"""
Convolutional encoder network

Author:  Hamish Morgan
Date:    02/07/2022
License: BSD
"""


from typing import Tuple

import functools
import sys

import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

from tensorflow.keras import layers

from hml.architectures.convolutional.blocks import (
    Conv2dBlock,
)
from hml.architectures.convolutional.constants import (
    LATENT_SHAPE_SQUARE,
    LATENT_SHAPE_WIDE,
    LATENT_SHAPE_ULTRAWIDE,
)


class Encoder(tf.keras.layers.Layer):
    """
    Convolutional encoder

    Encodes images as a multidimensional probability distribution, to form part of a
    variational autoencoder.
    """

    def __init__(
        self,
        latent_dim: int = 256,
        input_shape: Tuple[int, int, int] = (1152, 2048, 3),
        latent_shape: Tuple[int, int] = LATENT_SHAPE_WIDE,
        strides: int = 2,
        conv_filters: int = 128,
        repeat_layers: int = 0,
    ) -> "Encoder":
        """
        Construct the encoder

        Args:
            latent_dim: Dimension of posterior distribution generated by encoder
            input_shape: Shape of inputs to encoder
            conv_filters: Number of filters in each non-output conv layer

        Returns:
            Constructed encoder
        """
        super().__init__()

        kernel_initializer = tf.keras.initializers.RandomNormal(stddev=0.02)
        leaky_relu = functools.partial(tf.nn.leaky_relu, alpha=0.2)

        # Start with input layer
        self.input_ = layers.InputLayer(input_shape=input_shape)
        self.conv_layers_ = []
        shape = np.array(input_shape[:2], dtype=np.int64)

        # Add strided convs until output feature map is at desired latent shape
        while shape[0] > latent_shape[0] and shape[1] > latent_shape[1]:
            self.conv_layers_.append(
                Conv2dBlock(
                    filters=conv_filters,
                    kernel_size=5,
                    activation=leaky_relu,
                    kernel_initializer=kernel_initializer,
                    batch_norm=False,
                    regularise=0,
                    strides=strides,
                )
            )
            shape = (shape / strides).astype(int)

            # Optionally add more conv layers with no upscaling
            for _ in range(repeat_layers):
                self.conv_layers_.append(
                    Conv2dBlock(
                        filters=conv_filters,
                        kernel_size=5,
                        activation=leaky_relu,
                        kernel_initializer=kernel_initializer,
                        batch_norm=False,
                        regularise=0,
                        strides=1,
                    )
                )

        # Verify latent shape
        if tuple(shape) != latent_shape[:2]:
            print(
                f"Latent shape not achievable, requested {latent_shape}, achieved {shape}",
                file=sys.stderr,
            )

        # Output layers
        self.flattened = layers.Flatten()
        self.loc = layers.Dense(units=latent_dim, activation=None)
        self.scale = layers.Dense(units=latent_dim, activation=tf.nn.softplus)

    def call(self, inputs: tf.Tensor, training=False):
        """
        Produce a posterior distribution when called

        Args:
            inputs: Input images to be encoded
            training: True if we are training, False otherwise

        Returns:
            Posterior distribution (from encoder outputs)
        """
        # Input layer
        h = self.input_(inputs)

        # Pass through conv layers
        for layer in self.conv_layers_:
            h = layer(h, training=training)

        # Flatten
        hf = self.flattened(h, training=training)

        # Generate distribution location and scale from dense layers
        loc = self.loc(hf, training=training)
        scale = self.scale(hf, training=training)

        # Construct posterior
        self.posterior = tfp.distributions.MultivariateNormalDiag(
            loc=loc, scale_diag=scale
        )

        return self.posterior
