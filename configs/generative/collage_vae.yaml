#! /usr/bin/env generative generate --from-config


# Train/generate from config


# Model class (<class> in hml.models.<class>)
model_class: vae

# Args to model class constructor
__init__:

  # Size of latent vector in the middle of the network
  latent_dim: 16

  # Training data resolution
  input_shape: [1536, 2560, 3]

  # Depth of each conv layer
  conv_filters: 128

  # Strides of convolution layers
  strides: 2

  # Restore from checkpoint (no restore if null)
  checkpoint: null

  # Number of epochs between saves
  save_frequency: 200

# Args to .train() method of model class
train:

  # Experiment (output) directory
  model_dir: "/mnt/storage/ml/models/CollageVAE.eboy"

  # Path to training dataset
  train_path: "/mnt/storage/ml/data/pixel-art/eboy"

  # Path to validation dataset (only relevant for autoencoders)
  val_path: "/mnt/storage/ml/data/pixel-art/val"

  # Number of epochs to train for
  epochs: 100

  # Training batch size
  batch_size: 1

  # Number of examples to generate per epoch
  num_examples_to_generate: 1

  # Don't die if code is not committed
  debug: false
  
# Args to .generate() method of model class
generate:

  # Path to grayscale image to use as input to decoder
  decoder_input: null

  # Save images instead of displaying
  save_output: false

  # Number of images to generate
  num_generations: null

  # # Optional postprocessing funciton to apply to image (as numpy array)
  # postproc_func: Optional[Callable] = collage_vae_contrast_sharpen,
  
  
